from nltk.tokenize import word_tokenize
from datasets import Dataset
from tqdm.notebook import tqdm
import nltk
nltk.download('punkt')

from transformers import (
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    Trainer,
    TrainingArguments,
)
import numpy as np
import argparse
import json
import logging

text_column_name = 'tokens'
label_column_name = 'ner_tags'


parser = argparse.ArgumentParser(description='Predicting')
parser.add_argument('-model_path', dest='model_path', type=str, help='HuggingFace Model Path for Prediction', required=True)
parser.add_argument('-input_file', dest='input_file', type=str, help='Txt file for input', required=True)
parser.add_argument('-output_file', dest='output_file', type=str, help='Output file', required=True)
parser.add_argument('-batch_size', dest='batch_size', type=int, help='Predict Batch Size', default=32)
parser.add_argument('-max_length', dest='max_length', type=int, help='Sequence Input/Output Length', default=512)
parser.add_argument('-padding', dest='padding', type=int, help='Padding for Tokenizer', default='max_length')
parser.add_argument('-logging', dest='logging', type=bool, help='Set Logging', default=True)
parser.add_argument('-fp16', dest='fp16', type=bool, help='FP16', default=True)



args = parser.parse_args()

model_path = args.model_path
input_file = args.input_file
output_file = args.output_file
batch_size = args.batch_size
max_seq_length = args.max_length
fp16 = args.fp16
padding = args.padding

logger = logging.getLogger()
logger.setLevel(logging.INFO if args.logging else logging.NOTSET)


logger.info(f"==== Init pretrained model from Hugging Face's {model_path} ====")
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
model = AutoModelForTokenClassification.from_pretrained(model_path)

model.to("cuda")

# Data collator
data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if fp16 else None)

label_to_id = model.label2id
label_list = label_to_id.keys()

# Whether to put the label for one word on all tokens of generated by that word or just on the 
# one (in which case the other tokens will have a padding index).
label_all_tokens = False


# Tokenize all texts and align the labels with them.
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples[text_column_name],
        padding=padding,
        truncation=True,
        max_length=max_seq_length,
        # We use this argument because the texts in our dataset are lists of words (with a label for each word).
        is_split_into_words=True,
    )
    labels = []
    word_ids_list = []
    for i, label in enumerate(examples[label_column_name]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        word_ids_list.append(word_ids)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label_to_id[label[word_idx]])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label_to_id[label[word_idx]] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    tokenized_inputs["word_ids"] = word_ids_list
    return tokenized_inputs



def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_predictions, true_labels

def decoded_predictions(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return {'predictions': true_predictions}




logger.info(f"==== Running Word Tokenize for {input_file} ====")
with open(input_file, 'r', encoding='utf-8') as file:
  words_ = []
  ner_tags_ = []
  id_ = []
  for index, line in enumerate(tqdm(file, total=100000)):
    line = line.strip()
    words = word_tokenize(line)
    words_.append(words)
    ner_tags_.append(['O']*len(words))
    id_.append(index)


logger.info(f"==== Running tokenize_and_align_labels for {input_file} ====")
dataset = Dataset.from_dict({'tokens': words_, 'ner_tags': ner_tags_, 'id':id_})
predict_dataset = dataset.map(
                tokenize_and_align_labels,
                batched=True,
                num_proc=2,
                desc=f"Running tokenizer on Pubmed dataset, file: {input_file}",
                remove_columns = dataset.column_names
            )


args = TrainingArguments(output_dir='tmp_trainer', per_device_eval_batch_size=batch_size, fp16=fp16, fp16_full_eval=fp16)
trainer = Trainer(
    model=model,
    args=args,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=decoded_predictions,
)

logger.info("==== Start Predicting ====")
results = trainer.predict(predict_dataset)
predictions = results.metrics['predictions']


logger.info("==== Finished Predicting ====")
logger.info(f"==== Writing result to {output_file} ====")
dataset = dataset.remove_columns('id')
with open(output_file, 'w', encoding='utf-8') as out_file:
    for line, p in zip(dataset, predictions):
        line['ner_tags'] = p
        json.dump(line, out_file)
        out_file.write('\n')